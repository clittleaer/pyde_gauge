PYde gauge: Routines for working with TG data in support of comparison with model data

Capabilities -- available in some form

1. Importing monthly tide gauges from PSMSL: all conditional on importing .csv time series of monthly RLR data from PSMSL using their MATLAB interface. 
1. Monthly processing removing mean seasonal.
2. Filtering/Infilling/Sorting along coastlines
3. IB “Correcting” (ERA-5 for now)
Removal of global mean from all tide gauges
7. Filtering problematic gauges (right now, using momlevel threshold)

Altimetry:
extraction of closest point (using momlevel)
removal of global mean
standard time series analysis techniques, as for TG's
pseudo TG code, to sample coastal points for gridded datasets

CESM FOSI runs:
        * Processing scripts (TG/ALT/LR/HR)  - except for gridded/altimetry
        * Analysis example — just TG/ALT
    * Global processing and analysis scripts — monthly -> seasonal cycle, annual mean -> other capabilities (remove GMSL), tide gauges + altimetry. 
    * Metrics. Annual mean variability seasonal cycle trend and mean state. Philosophy: unclear which are most relevance to future changes.
    
    Analysis from CESM HR/LR paper in rep
    
    6. wavelets/power spectra/standard statistical analyses
    
Datasets -- available in some form
        * Datasets 
            * TG (PSMSL)
            * Altimetry (MEASURES 1/6, global mean)
            * Global mean 93-(MEASURES)
            * CESM FOSI LR/HR: point and gridded, regrinding
            * SEANOE dataset

Capabilities -- desired
5.wrap and/or recode RLR script from PSMSL
Long-period tide “corrections”
Uncertainty in corrections (multiple datasets)

4. Correcting for “global mean” terms, including fingerprints (Using Fredrikse et al. 2021 Dataset), and VLM (using XXX)
6. careful vetting of coastal locations where it makes sense to compare with models. Ideally, these are locations right along the coastline and are not nestled back in and embayments or upriver.  Would you happen to have such a list already for US or global locations? Although there are probably a few clear TGs to avoid, sometimes it is not very clear. And you might want to avoid them for other reasons, e.g. VLM or gauge-related issues. 
My -- maybe idealistic -- vision is to have this be determined by the spatial coherence of TGs (which of course will be determined by the spatial/temporal scale you care about!).
Clustering/spatial averaging.
Time mean

Datasets -- desired
            * TG (UHSLC): (I haven’t touched high frequency (sub-monthly) data in a while.)
            * TG NOAA
            * Altimetry (DUACS)
            * CESM coupled
            * MOM6
            * Generic CMIP6

For momlevel
 
* I agree that knowing the cell depth would be useful.  The analysis is fast, so one could simply repeat the analysis by passing an array of "deptho" to get the depth at the selected locations.
* The location function constructs a ball tree from the Scikit-learn package to find the nearest location.  The call from momlevel to Scikit could be adapted to return all of the nearest neighbor points within a threshold to compute a mean/variance.  A modest amount of work is needed, but it's not an intractable problem.